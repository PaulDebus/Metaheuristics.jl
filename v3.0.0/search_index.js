var documenterSearchIndex = {"docs":
[{"location":"api/#API-References","page":"API References","title":"API References","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"optimize","category":"page"},{"location":"api/#Metaheuristics.optimize","page":"API References","title":"Metaheuristics.optimize","text":"  optimize(\n        f::Function, # objective function\n        bounds::AbstractMatrix,\n        method::AbstractAlgorithm = ECA();\n        logger::Function = (status) -> nothing,\n  )\n\nMinimize a n-dimensional function f with domain bounds (2×n matrix) using method = ECA() by default.\n\nExample\n\nMinimize f(x) = Σx² where x ∈ [-10, 10]³.\n\nSolution:\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> result = optimize(f, bounds)\n+=========== RESULT ==========+\n| Iter.: 1008\n| f(x) = 6.48646e-163\n| solution.x = [-4.054471688602619e-82, 4.2565448859996416e-82, 5.505242086898758e-82]\n| f calls: 21187\n| Total time: 0.1231 s\n+============================+\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"State","category":"page"},{"location":"api/#Metaheuristics.State","page":"API References","title":"Metaheuristics.State","text":"State datatype\n\nState is used to store the current metaheuristic status. In fact, the optimize function returns a State.\n\nbest_sol Stores the best solution found so far.\npopulation is an Array{typeof(best_sol)} for population-based algorithms.\nf_calls is the number of objective functions evaluations.\ng_calls  is the number of inequality constraints evaluations.\nh_calls is the number of equality constraints evaluations.\niteration is the current iteration.\nsuccess_rate percentage of new generated solutions better that their parents. \nconvergence used save the State at each iteration.\nstart_time saves the time() before the optimization proccess.\nfinal_time saves the time() after the optimization proccess.\nstop if true, then stops the optimization proccess.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds)\n+=========== RESULT ==========+\n| Iter.: 1009\n| f(x) = 7.16271e-163\n| solution.x = [-7.691251412064516e-83, 1.0826961235605951e-82, -8.358428300092186e-82]\n| f calls: 21190\n| Total time: 0.2526 s\n+============================+\n\njulia> minimum(state)\n7.162710802659093e-163\n\njulia> minimizer(state)\n3-element Array{Float64,1}:\n -7.691251412064516e-83\n  1.0826961235605951e-82\n -8.358428300092186e-82\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Information","category":"page"},{"location":"api/#Metaheuristics.Information","page":"API References","title":"Metaheuristics.Information","text":"Information Structure\n\nInformation can be used to store the true optimum in order to stop a metaheuristic early.\n\nProperties:\n\nf_optimum known minimum.\nx_optimum known minimizer.\n\nIf Options is provided, then optimize will stop when |f(x) - f(x_optimum)| < Options.f_tol or ‖ x - x_optimum ‖ < Options.x_tol (euclidean distance).\n\nExample\n\nIf you want an approximation to the minimum with accuracy of 1e-3 (|f(x) - f(x*)| < 1e-3), then you may use Information.\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> information = Information(f_optimum = 0.0)\nInformation(0.0, Float64[])\n\njulia> options = Options(f_tol = 1e-3)\nOptions(0.0, 0.001, 0.0, 0.0, 1000.0, 0.0, 0.0, 0, false, true, false, :minimize)\n\njulia> state = optimize(f, bounds, ECA(information=information, options=options))\n+=========== RESULT ==========+\n| Iter.: 22\n| f(x) = 0.000650243\n| solution.x = [0.022811671589729583, 0.007052331140376011, -0.008951836265056107]\n| f calls: 474\n| Total time: 0.0106 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Options","category":"page"},{"location":"api/#Metaheuristics.Options","page":"API References","title":"Metaheuristics.Options","text":"Options(;\n    x_tol::Real = 0.0,\n    f_tol::Real = 0.0,\n    g_tol::Real = 0.0,\n    h_tol::Real = 0.0,\n    f_calls_limit::Real = 0,\n    g_calls_limit::Real = 0,\n    h_calls_limit::Real = 0,\n    time_limit::Real = Inf,\n    iterations::Int = 0,\n    store_convergence::Bool = false,\n    debug::Bool = false,\n    seed = rand(UInt)\n)\n\nOptions stores common settings for metaheuristics such as the maximum number of iterations debug options, maximum number of function evaluations, etc.\n\nMain properties:\n\nx_tol tolerance to the true minimizer if specified in Information.\nf_tol tolerance to the true minimum if specified in Information.\nf_calls_limit is the maximum number of function evaluations limit.\ntime_limit is the maximum time that optimize can spend in seconds.\niterations is the maximum number iterationn permited.\nstore_convergence if true, then push the current State in State.convergence at each generation/iteration\ndebug if true, then optimize function reports the current State (and interest information) for each iterations.\nseed non-negative integer for the random generator seed.\n\nExample\n\njulia> options = Options(f_calls_limit = 1000, debug=true, seed=1)\nOptions(0.0, 0.0, 0.0, 0.0, 1000.0, 0.0, 0.0, 0, false, true, true, :minimize, 0x0000000000000001)\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds, ECA(options=options))\n[ Info: Initializing population...\n[ Info: Starting main loop...\n+=========== RESULT ==========+\n| Iter.: 1\n| f(x) = 6.97287\n| solution.x = [-2.3628796262231875, -0.6781207370770752, -0.9642728360479853]\n| f calls: 42\n| Total time: 0.0004 s\n+============================+\n\n...\n\n[ Info: Stopped since call_limit was met.\n+=========== RESULT ==========+\n| Iter.: 47\n| f(x) = 1.56768e-08\n| solution.x = [-2.2626761322304715e-5, -9.838697194048792e-5, 7.405966506272336e-5]\n| f calls: 1000\n| Total time: 0.0313 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"convergence","category":"page"},{"location":"api/#Metaheuristics.convergence","page":"API References","title":"Metaheuristics.convergence","text":"convergence(state)\n\nget the data (touple with the number of function evaluations and fuction values) to plot the convergence graph. \n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds, ECA(options=Options(store_convergence=true)))\n+=========== RESULT ==========+\n| Iter.: 1022\n| f(x) = 7.95324e-163\n| solution.x = [-7.782044850211721e-82, 3.590044165897827e-82, -2.4665318114710003e-82]\n| f calls: 21469\n| Total time: 0.3300 s\n+============================+\n\njulia> n_fes, fxs = convergence(state);\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"minimizer","category":"page"},{"location":"api/#Metaheuristics.minimizer","page":"API References","title":"Metaheuristics.minimizer","text":"minimizer(state)\n\nReturns the approximation to the minimizer (argmin f(x)) stored in state.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"minimum(state::State)","category":"page"},{"location":"api/#Base.minimum-Tuple{State}","page":"API References","title":"Base.minimum","text":"minimum(state::Metaheuristics.State)\n\nReturns the approximation to the minimum (min f(x)) stored in state.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API References","title":"API References","text":"positions","category":"page"},{"location":"api/#Metaheuristics.positions","page":"API References","title":"Metaheuristics.positions","text":"positions(state)\n\nIf state.population has N solutions, then returns a N×d Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"fvals","category":"page"},{"location":"api/#Metaheuristics.fvals","page":"API References","title":"Metaheuristics.fvals","text":"fvals(state)\n\nIf state.population has N solutions, then returns a Vector with the  objective function values from items in state.population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"nfes","category":"page"},{"location":"api/#Metaheuristics.nfes","page":"API References","title":"Metaheuristics.nfes","text":"nfes(state)\n\nget the number of function evaluations.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"pareto_front(st::State)","category":"page"},{"location":"api/#Metaheuristics.pareto_front-Tuple{State}","page":"API References","title":"Metaheuristics.pareto_front","text":"pareto_front(state::State)\n\nReturns the non-dominated solutions in state.population.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API References","title":"API References","text":"pareto_front(st::Array)","category":"page"},{"location":"api/#Metaheuristics.pareto_front-Tuple{Array}","page":"API References","title":"Metaheuristics.pareto_front","text":"pareto_front(population::Array)\n\nReturns non-dominated solutions.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.create_child","category":"page"},{"location":"api/#Metaheuristics.create_child","page":"API References","title":"Metaheuristics.create_child","text":"Metaheuristics.create_child(x, fx)\n\nConstructor for a solution depending on the result of fx.\n\nExample\n\njulia> import Metaheuristics\n\njulia> Metaheuristics.create_child(rand(3), 1.0)\n| f(x) = 1\n| solution.x = [0.2700437125780806, 0.5233263210622989, 0.12871108215859772]\n\njulia> Metaheuristics.create_child(rand(3), (1.0, [2.0, 0.2], [3.0, 0.3]))\n| f(x) = 1\n| g(x) = [2.0, 0.2]\n| h(x) = [3.0, 0.3]\n| x = [0.9881102595664819, 0.4816273348099591, 0.7742585077942159]\n\njulia> Metaheuristics.create_child(rand(3), ([-1, -2.0], [2.0, 0.2], [3.0, 0.3]))\n| f(x) = [-1.0, -2.0]\n| g(x) = [2.0, 0.2]\n| h(x) = [3.0, 0.3]\n| x = [0.23983577719146854, 0.3611544510766811, 0.7998754930109109]\n\njulia> population = [ Metaheuristics.create_child(rand(2), (randn(2),  randn(2), rand(2))) for i = 1:100  ]\n                           F space\n          ┌────────────────────────────────────────┐ \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⠂⠀⠀⠀⠀⠀⡇⠈⡀⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠘⠀⡇⠀⠀⠘⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠂⠀⠂⠀⠀⢀⠠⠐⠀⡇⠄⠁⠀⠀⠀⡀⠀⢁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠂⢈⠀⠈⡇⠀⡐⠃⠀⠄⠄⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠄⢐⠠⠀⡄⠀⠀⡇⠀⠂⠈⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠉⠉⠉⠉⠋⠉⠉⠉⠉⠉⠉⠙⢉⠉⠙⠉⠉⡏⠉⠉⠩⠋⠉⠩⠉⠉⠉⡉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n   f_2    │⠀⠀⠀⠀⠀⡀⠀⠀⠀⠄⠀⠀⡀⠀⠀⠂⠀⡇⠀⠀⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⠐⡇⠠⠀⠀⠀⠈⢀⠄⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠂⠀⠄⠀⡀⠀⠂⡇⠐⠘⠈⠂⠀⠈⡀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠄⠀⠀⠀⠀⠀⠂⠀⠂⠀⠀⡇⠀⠈⢀⠐⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⠀⠀⠀⠀⢁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n       -3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          -3                                       4\n                             f_1\n\n\n\n\n\n","category":"function"},{"location":"problems/#Problems","page":"Problems","title":"Problems","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":"Benchmark Test Problems for numerical optimization.","category":"page"},{"location":"problems/","page":"Problems","title":"Problems","text":" Metaheuristics.TestProblems.get_problem","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.get_problem","page":"Problems","title":"Metaheuristics.TestProblems.get_problem","text":"get_problem(problem)\n\nReturns a 3-tuple with the objective function, the bounds and 100 Pareto solutions for multiobjective optimization problems or the optimal solutions for (box)constrained optimization problems.\n\nHere, problem can be one of the following symbols:\n\n:sphere\n:discus\n:rastrigin\n:ZDT1\n:ZDT2\n:ZDT3\n:ZDT4\n:ZDT6\n:DTLZ2\n\nExample\n\njulia> import Metaheuristics: TestProblems, optimize\n\njulia> f, bounds, pareto_solutions = TestProblems.get_problem(:ZDT3);\n\n\njulia> bounds\n2×30 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\njulia> pareto_solutions\n                           F space\n          ┌────────────────────────────────────────┐ \n        1 │⢅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠈⢢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠙⠒⠀⠀⠀⠀⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠘⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠱⠄⠀⠀⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠬⡦⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠂⠀⠀⠀⠀⠀⠀⢢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢧⡀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡆⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠀⠀│ \n       -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          0                                      0.9\n                             f_1\n\njulia> optimize(f, bounds)\n+=========== RESULT ==========+\n| Iter.: 1428\nPopulation\n                           F space\n          ┌────────────────────────────────────────┐ \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠈⠳⠤⠀⠀⠀⠀⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2    │⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠳⠄⠀⠀⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠛⠍⠉⠉⠉⠉⠉⠉⢭⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠁⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢧⠀⠀│ \n       -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          0                                      0.9\n                             f_1\n\nBest solution(s)\n                           F space\n          ┌────────────────────────────────────────┐ \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠈⠳⠤⠀⠀⠀⠀⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2    │⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠳⠄⠀⠀⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠛⠍⠉⠉⠉⠉⠉⠉⢭⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠁⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢧⠀⠀│ \n       -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          0                                      0.9\n                             f_1\n\n| f calls: 300090\n| feasibles: 210 / 210 in final population\n+============================+\n\n\n\n\n\n","category":"function"},{"location":"problems/#Box-constrained-Optimization","page":"Problems","title":"Box-constrained Optimization","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":" Metaheuristics.TestProblems.sphere","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.sphere","page":"Problems","title":"Metaheuristics.TestProblems.sphere","text":"sphere(D)\n\nThe well-known D-dimensional Sphere function.\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.discus","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.discus","page":"Problems","title":"Metaheuristics.TestProblems.discus","text":"discus(D)\n\nThe well-known D-dimensional Discus function.\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.rastrigin","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.rastrigin","page":"Problems","title":"Metaheuristics.TestProblems.rastrigin","text":"rastrigin(D)\n\nThe well-known D-dimensional Rastrigin function.\n\n\n\n\n\n","category":"function"},{"location":"problems/#Constrained-Optimization","page":"Problems","title":"Constrained Optimization","text":"","category":"section"},{"location":"problems/#Multi-objective-Optimization","page":"Problems","title":"Multi-objective Optimization","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT1","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT1","page":"Problems","title":"Metaheuristics.TestProblems.ZDT1","text":"ZDT1(D, n_solutions)\n\nZDT1 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT2","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT2","page":"Problems","title":"Metaheuristics.TestProblems.ZDT2","text":"ZDT2(D, n_solutions)\n\nZDT2 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nnonconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT3","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT3","page":"Problems","title":"Metaheuristics.TestProblems.ZDT3","text":"ZDT3(D, n_solutions)\n\nZDT3 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nconvex disconected\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT4","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT4","page":"Problems","title":"Metaheuristics.TestProblems.ZDT4","text":"ZDT4(D, n_solutions)\n\nZDT4 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nnonconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT6","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT6","page":"Problems","title":"Metaheuristics.TestProblems.ZDT6","text":"ZDT6(D, n_solutions)\n\nZDT6 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of Pareto solutions.\n\nMain properties:\n\nnonconvex\nnon-uniformly spaced\n\n\n\n\n\n","category":"function"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Please, be free to send me your PR, issue or any comment about this package for Julia.","category":"page"},{"location":"contributing/#Algorithm-Structure","page":"Contributing","title":"Algorithm Structure","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you call the optimize function, the following steps are carried out:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Initialization: status = initialize!(parameters, problem, information, options) this function should initialize a State with population members according to the parameters provided.\nMain optimization loop: while status.stop == false do\nupdate population, parameters via update_state!(problem, engine, parameters, status, information, options, iteration), and \nmainly set status.stop = engine.stop_criteria(status, information, options)\nWhen the loop in step 2 beaks, then a final function is called final_stage! in order to update or refine the final state, e.g., delete infeasible solutions in population, get non-dominated solutions, etc. ","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Initialization:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"function initialize!(\n                parameters::AbstractParameters,\n                problem,\n                information,\n                options,\n                args...;\n                kargs...\n        )\n    # initialize parameters, population, etc.\n    # return the status\n    return State(0.0, zeros(0))\nend","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Optimization Process: In this step, the State is updated using the following function which is called at each iteration/generation.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"function update_state!(\n        status,\n        parameters::AbstractParameters,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n)\n    # update any element in State \n    return\nend","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Final Step:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"function final_stage(\n        status,\n        parameters::AbstractParameters,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n)\n    return\nend","category":"page"},{"location":"contributing/#Parameters","page":"Contributing","title":"Parameters","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Any proposed algorithm, let's say \"XYZ\", uses different parameters, then it is suggested to store them in a structure, e.g.:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"# structure with algorithm parameters\nmutable struct XYZ <: AbstractParameters\n    N::Int # population size\n    p_crossover::Float64 # crossover probability\n    p_mutation::Float64 # mutation probability\nend\n\n# constructor like\nfunction XYZ(;N = 0, p_crossover = 0.9, p_mutation = 0.1)\n    parameters = XYZ(N, p_crossover, p_mutation)\n\n    Algorithm(\n        parameters,\n        information = information,\n        options = options,\n    )\nend","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"TODO Tutorial for creating and updating existent metaheuristics in this package.","category":"page"},{"location":"indicators/#Performance-Indicators","page":"Performance Idicators","title":"Performance Indicators","text":"","category":"section"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators","text":"PerformanceIndicators\n\nThis module includes performance indicators to assess evolutionary multi-objective optimization algorithms.\n\ngd Generational Distance\nigd Inverted Generational Distance\ngd_plus Generational Distance plus\nigd_plus Inverted Generational Distance plus\n\nExample\n\njulia> import Metaheuristics: PerformanceIndicators, TestProblems\n\njulia> A = [ collect(1:3) collect(1:3) ]\n3×2 Array{Int64,2}:\n 1  1\n 2  2\n 3  3\n\njulia> B = A .- 1\n3×2 Array{Int64,2}:\n 0  0\n 1  1\n 2  2\n\njulia> PerformanceIndicators.gd(A, B)\n0.47140452079103173\n\njulia> f, bounds, front = TestProblems.get_problem(:ZDT1);\n\njulia> front\n                          F space\n         ┌────────────────────────────────────────┐ \n       1 │⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠈⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠈⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠈⠢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2   │⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠲⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠒⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⠢⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠢⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠢⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠢⢤⣀⠀⠀⠀⠀⠀│ \n       0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⠢⢄⣀│ \n         └────────────────────────────────────────┘ \n         0                                        1\n                            f_1\n\njulia> PerformanceIndicators.igd_plus(front, front)\n0.0\n\n\n\n\n\n","category":"module"},{"location":"indicators/#Multi-objective","page":"Performance Idicators","title":"Multi-objective","text":"","category":"section"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators.gd","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.gd","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators.gd","text":"gd(front, true_pareto_front; p = 1)\n\nReturns the Generational Distance.\n\nParameters\n\nfront and true_pareto_front can be: \t- N×m matrix where N is the number of points and m is the number of objectives.  \t- State \t- Array{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators.gd_plus","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.gd_plus","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators.gd_plus","text":"gd_plus(front, true_pareto_front; p = 1)\n\nReturns the Generational Distance Plus.\n\nParameters\n\nfront and true_pareto_front can be: \t- N×m matrix where N is the number of points and m is the number of objectives.  \t- State \t- Array{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators.igd","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.igd","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators.igd","text":"igd(front, true_pareto_front; p = 1)\n\nReturns the Inverted Generational Distance.\n\nParameters\n\nfront and true_pareto_front can be: \t- N×m matrix where N is the number of points and m is the number of objectives.  \t- State \t- Array{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators.igd_plus","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.igd_plus","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators.igd_plus","text":"igd_plus(front, true_pareto_front; p = 1)\n\nReturns the Inverted Generational Distance Plus.\n\nParameters\n\nfront and true_pareto_front can be: \t- N×m matrix where N is the number of points and m is the number of objectives.  \t- State \t- Array{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators.spacing","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.spacing","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators.spacing","text":"spacing(A)\n\nComputes the Schott spacing indicator. spacing(A) == 0 means that vectors in A are uniformly distributed.\n\n\n\n\n\n","category":"function"},{"location":"indicators/","page":"Performance Idicators","title":"Performance Idicators","text":" Metaheuristics.PerformanceIndicators.covering","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.covering","page":"Performance Idicators","title":"Metaheuristics.PerformanceIndicators.covering","text":"covering(A, B)\n\nComputes the covering indicator (percentage of vectors in B that are dominated by vectors in A) from two sets with non-dominated solutions.\n\nA and B with size (n, m) where n is number of samples and m is the vector dimension.\n\nNote that covering(A, B) == 1 means that all solutions in B are dominated by those in A. Moreover, covering(A, B) != covering(B, A) in general.\n\nIf A::State and B::State, the computes covering(A.population, B.population) after ignoring dominated solutions in each set.\n\n\n\n\n\n","category":"function"},{"location":"visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Present the results using fancy plots is an important part of solving optimization problems. In this part, we use the Plots.jl package which can be installed via de Pkg prompt within Julia:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Type ] and then:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"pkg> add Plots","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Or:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"julia> import Pkg; Pkg.add(\"Plots\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Once Plots is installed on your Julia distribution, you will be able to reproduce the  following examples.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Assume you want to solve the following minimization problem.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Rastrigin Surface)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Minimize:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"f(x) = 10D + sum_i=1^D  x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"where xin-5 5^D, i.e., -5 leq x_i leq 5 for i=1ldotsD. D is the dimension number, assume D=10.","category":"page"},{"location":"visualization/#Population-Distribution","page":"Visualization","title":"Population Distribution","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Let's solve the above optimization problem and plot the resulting population (projecting two specific dimensions).","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\n# positions in matrix NxD \nX = positions(result)\n\nscatter(X[:,1], X[:,2], label=\"Population\")\n\nx = minimizer(result)\nscatter!(x[1:1], x[2:2], label=\"Best solution\")\n\n\n# (optional) save figure\nsavefig(\"final-population.png\")\n","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"If your optimization problem is scalable, then you also can plot level curves. In this case, let's assume that D=2.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 2\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\n# positions in matrix NxD \nX = positions(result)\n\nxy = range(-5, 5, length=100)\ncontour(xy, xy, (a,b) -> f([a, b]))\n\nscatter!(X[:,1], X[:,2], label=\"Population\")\n\nx = minimizer(result)\nscatter!(x[1:1], x[2:2], label=\"Best solution\")\n\n\n# (optional) save figure\nsavefig(\"final-population-contour.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Objective-Function-Values","page":"Visualization","title":"Objective Function Values","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Metaheuristics implements some methods to obtain the objective function values (fitness) from the solutions in resulting population. One of the most useful method is fvals. In this case, let's use PSO.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, PSO(options=options))\n\nf_values = fvals(result)\nplot(f_values)\n\n# (optional) save figure\nsavefig(\"fvals.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Convergence","page":"Visualization","title":"Convergence","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Sometimes, it is useful to plot the convergence plot at the end of the optimization process. To do that, it is necessary to set store_convergence = true in Options. Metaheuristics implements a method called convergence.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1, store_convergence = true)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\nf_calls, best_f_value = convergence(result)\n\nplot(xlabel=\"f calls\", ylabel=\"fitness\", title=\"Convergence\")\nplot!(f_calls, best_f_value, label=\"ECA\")\n\n# (optional) save figure\nsavefig(\"convergence.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Convergence)","category":"page"},{"location":"visualization/#Animate-convergence","page":"Visualization","title":"Animate convergence","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Also, you can plot the population and convergence in the same figure.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1, store_convergence = true)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\nf_calls, best_f_value = convergence(result)\n\nanimation = @animate for i in 1:length(result.convergence)\n    l = @layout [a b]\n    p = plot( layout=l)\n\n    X = positions(result.convergence[i])\n    scatter!(p[1], X[:,1], X[:,2], label=\"\", xlim=(-5, 5), ylim=(-5,5), title=\"Population\")\n    x = minimizer(result.convergence[i])\n    scatter!(p[1], x[1:1], x[2:2], label=\"\")\n\n    # convergence\n    plot!(p[2], xlabel=\"Generation\", ylabel=\"fitness\", title=\"Gen: $i\")\n    plot!(p[2], 1:length(best_f_value), best_f_value, label=false)\n    plot!(p[2], 1:i, best_f_value[1:i], lw=3, label=false)\n    x = minimizer(result.convergence[i])\n    scatter!(p[2], [i], [minimum(result.convergence[i])], label=false)\nend\n\n# save in different formats\n# gif(animation, \"anim-convergence.gif\", fps=30)\nmp4(animation, \"anim-convergence.mp4\", fps=30)\n","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: )","category":"page"},{"location":"visualization/#Pareto-Front","page":"Visualization","title":"Pareto Front","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, NSGA2, TestProblems, pareto_front, Options\nusing Plots; gr()\n\nf, bounds, solutions = TestProblems.ZDT3();\n\nresult = optimize(f, bounds, NSGA2(options=Options(seed=0)))\n\nA = pareto_front(result)\nB = pareto_front(solutions)\n\nscatter(A[:, 1], A[:,2], label=\"NSGA-II\")\nplot!(B[:, 1], B[:,2], label=\"Parento Front\", lw=2)\nsavefig(\"pareto.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Live-Plotting","page":"Visualization","title":"Live Plotting","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"The optimize function has a keyword parameter named logger that contains a function pointer. Such function will receive the State at the end of each iteration in the main optimization loop.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, NSGA2, TestProblems, pareto_front, Options, fvals\nusing Plots; gr()\n\nf, bounds, solutions = TestProblems.ZDT3();\npf = pareto_front(solutions)\n\nlogger(st) = begin\n    A = fvals(st)\n    scatter(A[:, 1], A[:,2], label=\"NSGA-II\", title=\"Gen: $(st.iteration)\")\n    plot!(pf[:, 1], pf[:,2], label=\"Parento Front\", lw=2)\n    gui()\n    sleep(0.1)\nend\n\nresult = optimize(f, bounds, NSGA2(options=Options(seed=0)), logger=logger)\n","category":"page"},{"location":"#Metaheuristics-an-Intuitive-Package-for-Global-Optimization","page":"Introduction","title":"Metaheuristics - an Intuitive Package for Global Optimization","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Author: Jesus Mejía (@jmejia8)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"High performance algorithms for optimization purely coded in a high performance language.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: Build Status) (Image: Coverage Status) (Image: Doc)","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Optimization is one of the most common task in the scientific and industry field but real-world problems require high-performance algorithms to optimize non-differentiable, non-convex, dicontinuous functions. Different metaheuristics algorithms have been proposed to solve optimization problems but without strong assumptions about the objective function.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This package implements state-of-the-art metaheuristics algorithms for global optimization. The aim of this package is to provide easy to use (and fast) metaheuristics for numerical global optimization.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Open the Julia (Julia 1.1 or Later) REPL and press ] to open the Pkg prompt. To add this package, use the add command:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add Metaheuristics","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Or, equivalently, via the Pkg API:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> import Pkg; Pkg.add(\"Metaheuristics\")","category":"page"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Assume you want to solve the following minimization problem.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: Rastrigin Surface)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Minimize:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"f(x) = 10D + sum_i=1^D  x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"where xin-5 5^D, i.e., -5 leq x_i leq 5 for i=1ldotsD. D is the dimension number, assume D=10.","category":"page"},{"location":"#Solution","page":"Introduction","title":"Solution","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Firstly, import the Metaheuristics package:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using Metaheuristics","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Code the objective function:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Instantiate the bounds, note that bounds should be a 2times 10 Matrix where the first row corresponds to the lower bounds whilst the second row corresponds to the upper bounds.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"D = 10\nbounds = [-5ones(D) 5ones(D)]'","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Approximate the optimum using the function optimize.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"result = optimize(f, bounds)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Optimize returns a State datatype which contains some information about the approximation. For instance, you may use mainly two functions to obtain such approximation.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"@show minimum(result)\n@show minimizer(result)","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"After reading this tutorial you'll become an expert using Metaheuristics module.","category":"page"},{"location":"tutorial/#Minimization-Problem","page":"Tutorial","title":"Minimization Problem","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Assume you want to optimize the following minimization problem:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Minimize:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"f(x) = 10D + sum_i=1^D x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where xin -5 5^D, that is, each coordinate in x is between -5 and 5. Use D=10.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that the global optimum is obtained when x_i = 0 for all i. Thus, min f(x) = 0.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Objective function:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"f(x) = 10length(x) + sum( x.^2 - 10cos.(2pi*x) )","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Bounds:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"bounds = [-5ones(10) 5ones(10)]'","category":"page"},{"location":"tutorial/#Providing-Information","page":"Tutorial","title":"Providing Information","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since the optimum is known, then we can provide this information to the optimizer.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"information = Information(f_optimum = 0.0)","category":"page"},{"location":"tutorial/#Common-Settings","page":"Tutorial","title":"Common Settings","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Usually users could require to limit the number of generation/iteration or the number of function evaluations. To do that, let's assume that the metaheuristic should evaluate at most 9000D times the objective function. Moreover, since information is provided, then we can set the desired accuracy (f(x) - f(x^*) ) to 10^-5.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"options = Options(f_calls_limit = 9000*10, f_tol = 1e-5)","category":"page"},{"location":"tutorial/#Choose-a-Metaheuristic","page":"Tutorial","title":"Choose a Metaheuristic","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Metaheuristics.jl provides different metaheuristics for optimization such as Evolutionary Centers Algorithm (ECA), Differential Evolution (DE), Particle Swarm Optimization (PSO), etc. In this tutorial we will use ECA, but you can use another algorithm following but the same steps.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The metaheuristics accept its parameters but share two common and optional settings information and options.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"algorithm = ECA(information = information, options = options)","category":"page"},{"location":"tutorial/#Optimize","page":"Tutorial","title":"Optimize","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now, we are able to approximate the optimum. To do that is necessary to use the optimize function as follows:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"result = optimize(f, bounds, algorithm)","category":"page"},{"location":"tutorial/#Get-the-Results","page":"Tutorial","title":"Get the Results","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Once optimize stopped, then we can get the approximate solutions.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Approximated minimum:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"fx = minimum(result)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Approximated minimizer:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"x = minimizer(result)","category":"page"},{"location":"tutorial/#Get-Information-about-the-Resulting-Population","page":"Tutorial","title":"Get Information about the Resulting Population","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Sometimes is useful to analyze the resulting population (for population-based metaheuristics). To do that you can use fvals to get objective function evaluation and positions to get their positions.","category":"page"},{"location":"tutorial/#Bonus","page":"Tutorial","title":"Bonus","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We recommend you to save  your program in a function for performance purposes:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Metaheuristics\n\nfunction main()\n    # objective function\n    f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x) )\n    \n    # limits/bounds\n    bounds = [-5ones(10) 5ones(10)]'\n    \n    # information on the minimization problem\n    information = Information(f_optimum = 0.0)\n\n    # generic settings\n    options = Options(f_calls_limit = 9000*10, f_tol = 1e-5)\n    \n    # metaheuristic used to optimize\n    algorithm = ECA(information = information, options = options)\n\n    # start the minimization proccess\n    result = optimize(f, bounds, algorithm)\n\n    \n    fx = minimum(result)\n    x = minimizer(result)\n\n    @show fx\n    @show x\nend\n","category":"page"},{"location":"tutorial/#Summary","page":"Tutorial","title":"Summary","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now you are able to approximate global optimum solutions using Metaheuristics.","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"List of implemented metaheuristics.","category":"page"},{"location":"algorithms/#Evolutionary-Centers-Algorithm","page":"Algorithms","title":"Evolutionary Centers Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ECA","category":"page"},{"location":"algorithms/#Metaheuristics.ECA","page":"Algorithms","title":"Metaheuristics.ECA","text":"ECA(;\n    η_max = 2.0,\n    K = 7,\n    N = 0,\n    N_init = N,\n    p_exploit = 0.95,\n    p_bin = 0.02,\n    p_cr = Float64[],\n    adaptive = false,\n    resize_population = false,\n    information = Information(),\n    options = Options()\n)\n\nParameters for the metaheuristic ECA: step-size η_max,K is number of vectors to generate the center of mass, N is the population size.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ECA())\n\n+=========== RESULT ==========+\n| Iter.: 1021\n| f(x) = 1.68681e-163\n| solution.x = [2.5517634463667404e-82, -2.9182760041942484e-82, -1.3565584801935802e-82]\n| f calls: 21454\n| Total time: 0.0894 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ECA(N = 10, η_max = 1.0, K = 3))\n+=========== RESULT ==========+\n| Iter.: 1506\n| f(x) = 0.000172391\n| solution.x = [-6.340714627875324e-5, -0.004127226953894587, 0.012464071313908906]\n| f calls: 15069\n| Total time: 0.0531 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Differential-Evolution","page":"Algorithms","title":"Differential Evolution","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"DE","category":"page"},{"location":"algorithms/#Metaheuristics.DE","page":"Algorithms","title":"Metaheuristics.DE","text":"DE(;\n    N  = 0,\n    F  = 1.0,\n    CR = 0.9,\n    strategy = :rand1,\n    information = Information(),\n    options = Options()\n)\n\nParameters for Differential Evolution (DE) algorithm: step-size F,CR controlls the binomial crossover, N is the population size. The parameter trategy is related to the variation operator (:rand1, :rand2, :best1, :best2, :randToBest1).\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], DE())\n+=========== RESULT ==========+\n| Iter.: 437\n| f(x) = 0\n| solution.x = [0.0, 0.0, 0.0]\n| f calls: 13134\n| Total time: 0.3102 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], DE(N=50, F=1.5, CR=0.8))\n+=========== RESULT ==========+\n| Iter.: 599\n| f(x) = 9.02214e-25\n| solution.x = [-4.1003250484858545e-13, -6.090890160928905e-13, -6.025762626763004e-13]\n| f calls: 30000\n| Total time: 0.0616 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Particle-Swarm-Optimization","page":"Algorithms","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"PSO","category":"page"},{"location":"algorithms/#Metaheuristics.PSO","page":"Algorithms","title":"Metaheuristics.PSO","text":"PSO(;\n    N  = 0,\n    C1 = 2.0,\n    C2 = 2.0,\n    ω  = 0.8,\n    information = Information(),\n    options = Options()\n)\n\nParameters for Particle Swarm Optimization (PSO) algorithm: learning rates C1 and C2, N is the population size and ω controlls the inertia weight. \n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], PSO())\n+=========== RESULT ==========+\n| Iter.: 999\n| f(x) = 3.23944e-48\n| solution.x = [1.0698542573895642e-24, -1.4298101555926563e-24, -2.247029420442994e-25]\n| f calls: 30000\n| Total time: 0.4973 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], PSO(N = 100, C1=1.5, C2=1.5, ω = 0.7))\n+=========== RESULT ==========+\n| Iter.: 299\n| f(x) = 1.41505e-38\n| solution.x = [2.161357427851024e-20, -1.1599444038307776e-19, 1.5122345732802047e-20]\n| f calls: 30000\n| Total time: 0.2128 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Artificial-Bee-Colony","page":"Algorithms","title":"Artificial Bee Colony","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ABC","category":"page"},{"location":"algorithms/#Metaheuristics.ABC","page":"Algorithms","title":"Metaheuristics.ABC","text":"ABC(;\n    N = 50,\n    Ne = div(N+1, 2),\n    No = div(N+1, 2),\n    limit=10,\n    information = Information(),\n    options = Options()\n)\n\nABC implements the original parameters for the Artificial Bee Colony Algorithm. N is the population size, Ne is the number of employees, No is the number of outlookers bees. limit is related to the times that a solution is visited.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ABC())\n+=========== RESULT ==========+\n| Iter.: 593\n| f(x) = 3.54833e-25\n| solution.x = [3.448700205761237e-13, 4.805851037329074e-13, 7.025504722610375e-14]\n| f calls: 30019\n| Total time: 0.2323 s\n+============================+\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ABC(N = 80,  No = 20, Ne = 50, limit=5))\n+=========== RESULT ==========+\n| Iter.: 405\n| f(x) = 2.24846e-07\n| solution.x = [0.0002682351072804559, 0.00020460896416511776, 0.0003332131896109299]\n| f calls: 30043\n| Total time: 0.2652 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#MOEA/D-DE","page":"Algorithms","title":"MOEA/D-DE","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"MOEAD_DE","category":"page"},{"location":"algorithms/#Metaheuristics.MOEAD_DE","page":"Algorithms","title":"Metaheuristics.MOEAD_DE","text":"MOEAD_DE(weights)\n\nMOEAD_DE implements the original version of MOEA/D-DE. It uses the contraint handling method based on the sum of violations (for constrained optimizaton): g(x, λ, z) = max(λ .* abs.(fx - z)) + sum(max.(0, gx)) + sum(abs.(hx))\n\nTo use MOEAD_DE, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are the equality and inequality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nRef. Multiobjective Optimization Problems With Complicated Pareto Sets, MOEA/D and NSGA-II; Hui Li and Qingfu Zhang.\n\nExample\n\nAssume you want to solve the following optimizaton problem:\n\nMinimize:\n\nf(x) = (x_1, x_2)\n\nsubject to:\n\ng(x) = x_1^2 + x_2^2 - 1 ≤ 0\n\nx_1, x_2 ∈ [-1, 1]\n\nA solution can be:\n\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\nnobjectives = 2\nnpartitions = 100\n\n# reference points (Das and Dennis's method)\nweights = gen_ref_dirs(nobjectives, npartitions)\n\n# define the parameters\nmoead_de = MOEAD_DE(weights, options=Options(debug=false, iterations = 250))\n\n# optimize\nstatus_moead = optimize(f, bounds, moead_de)\n\n# show results\ndisplay(status_moead)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Gravitational-Search-Algorithm","page":"Algorithms","title":"Gravitational Search Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"CGSA","category":"page"},{"location":"algorithms/#Metaheuristics.CGSA","page":"Algorithms","title":"Metaheuristics.CGSA","text":"CGSA(;\n    N::Int    = 30,\n    chValueInitial::Real   = 20,\n    chaosIndex::Real   = 9,\n    ElitistCheck::Int    = 1,\n    Rpower::Int    = 1,\n    Rnorm::Int    = 2,\n    wMax::Real   = chValueInitial,\n    wMin::Real   = 1e-10,\n    information = Information(),\n    options = Options()\n)\n\nCGSA is an extension of the GSA algorithm but with Chaotic gravitational constants for the gravitational search algorithm.\n\nRef. Chaotic gravitational constants for the gravitational search algorithm. Applied Soft Computing 53 (2017): 407-419.\n\nParameters:\n\nN: Population size\nchValueInitial: Initial value for the chaos value\nchaosIndex: Integer 1 ≤ chaosIndex ≤ 10 is the function that model the chaos\nRpower: power related to the distance norm(x)^Rpower\nRnorm: is the value as in norm(x, Rnorm)\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], CGSA())\n+=========== RESULT ==========+\n| Iter.: 499\n| f(x) = 0.000235956\n| solution.x = [0.0028549782101697785, -0.0031385153631797724, 0.014763299731686608]\n| f calls: 15000\n| Total time: 0.1003 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], CGSA(N = 80, chaosIndex = 1))\n+=========== RESULT ==========+\n| Iter.: 499\n| f(x) = 0.000102054\n| solution.x = [0.00559987302269564, 0.00017535321765604905, 0.008406213942044265]\n| f calls: 40000\n| Total time: 0.5461 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Simulated-Annealing","page":"Algorithms","title":"Simulated Annealing","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SA","category":"page"},{"location":"algorithms/#Metaheuristics.SA","page":"Algorithms","title":"Metaheuristics.SA","text":"    SA(;\n        x_initial::Vector = zeros(0),\n        N::Int = 500,\n        tol_fun::Real= 1e-4,\n        information = Information(),\n        options = Options()\n    )\n\nParameters for the method of Simulated Annealing (Kirkpatrick et al., 1983).\n\nParameters:\n\nx_intial: Inital solution. If empty, then SA will generate a random one within the bounds.\nN: The number of test points per iteration.\ntol_fun: tolerance value for the Metropolis condition to accept or reject the test point as current point.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], SA())\n+=========== RESULT ==========+\n| Iter.: 60\n| f(x) = 2.84574e-73\n| solution.x = [-5.307880224731971e-37, -5.183298967486749e-38, 1.2301984439451926e-38]\n| f calls: 29502\n| Total time: 0.0465 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], SA(N = 100, x_initial = [1, 0.5, -1]))\n+=========== RESULT ==========+\n| Iter.: 300\n| f(x) = 1.29349e-70\n| solution.x = [-7.62307964668667e-36, 8.432089040013441e-36, -3.7077496015659554e-37]\n| f calls: 29902\n| Total time: 0.0466 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Whale-Optimization-Algorithm","page":"Algorithms","title":"Whale Optimization Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"WOA","category":"page"},{"location":"algorithms/#Metaheuristics.WOA","page":"Algorithms","title":"Metaheuristics.WOA","text":"WOA(;N = 30, information = Information(), options = Options())\n\nParameters for the Whale Optimization Algorithm. N is the population size (number of whales).\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], WOA())\n+=========== RESULT ==========+\n| Iter.: 499\n| f(x) = 4.56174e-104\n| solution.x = [-1.04059445339676e-52, 1.7743142412652892e-52, 5.750781222647098e-53]\n| f calls: 15000\n| Total time: 0.0844 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], WOA(N = 100))\n+=========== RESULT ==========+\n| Iter.: 499\n| f(x) = 1.29795e-147\n| solution.x = [1.306372696781744e-74, -3.017649118559932e-75, 3.3439182063846375e-74]\n| f calls: 50000\n| Total time: 0.1894 s\n+============================+\n\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NSGA-II","page":"Algorithms","title":"NSGA-II","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"NSGA2","category":"page"},{"location":"algorithms/#Metaheuristics.NSGA2","page":"Algorithms","title":"Metaheuristics.NSGA2","text":"function NSGA2(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    ε = eps(),\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-II.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\n\nTo use NSGA2, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `NSGA2()` for using default parameters)\nnsga2 = NSGA2(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, nsga2)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NSGA-III","page":"Algorithms","title":"NSGA-III","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"NSGA3","category":"page"},{"location":"algorithms/#Metaheuristics.NSGA3","page":"Algorithms","title":"Metaheuristics.NSGA3","text":"function NSGA3(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    ε = eps(),\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-III.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\n\nTo use NSGA3, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n\n# Objective function, bounds, and the True Pareto front\nf, bounds, pf = Metaheuristics.TestProblems.get_problem(:DTLZ2)\n\n\n# define the parameters (use `NSGA3()` for using default parameters)\nnsga3 = NSGA3(p_cr = 0.9)\n\n# optimize\nstatus = optimize(f, bounds, nsga3)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"}]
}
